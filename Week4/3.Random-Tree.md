1. This algorithm reduces the expense that Random Tree plagued by.
2. Pseudocode:
```
build_tree(data)
    if data.shape[0] == 1: return [leaf, data.y, NA, NA] 
    if all data.y same: return [leaf, data.y, NA, NA] 
    else
        determine random feature i to split on
        SplitVal = (data[random,i] + data[random,i]) / 2 
        lefttree = build_tree(data[data[:,i]<=SplitVal]) 
        righttree = build_tree(data[data[:,i]>SplitVal]) 
        root = [i, SplitVal, 1, lefttree.shape[0] + 1] 
        return (append(root, lefttree, righttree))
```
3. Choosing random features, although reduces time, it messes up our tree.
    - But there is a work around.
        - Use bagging to create multiple trees - an ensemble.
        - Use random data to train your models (bags).
4. Instead of calcualting median, random forest does something different
    - Choose random rows, twice.
    - Add them and the half that value.
5. These two random selections make it extrememly fast.