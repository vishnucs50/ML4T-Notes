1. How to determine the best feature?
    - For the root note, we find the feature that segregates the data the best. 
    - At each level we must choose the feature that does the seperation effectively.
    - We can use ***Entropy***, ***Correlation***, or ***Gini Index*** to calcualte the best feature.
    - For this project we use correlation.
    - The feature that is the most strongly correlated with the output helps us split the data effectively.
    - If different features have same correlation value:
        1. Choose one randomly.
        2. Or just choose the first one or the last one. 
2. This algorithm is somewhat expensive because:
    - First, here is the algorithm:
    ```
    build_tree(data)
        if data.shape[0] == 1: return [leaf, data.y, NA, NA] 
        if all data.y same: return [leaf, data.y, NA, NA] 
        else
            determine best feature i to split on
            SplitVal = data[:,i].median()
            lefttree = build_tree(data[data[:,i]<=SplitVal]) 
            righttree = build_tree(data[data[:,i]>SplitVal]) 
            root = [i, SplitVal, 1, lefttree.shape[0] + 1] 
            return (append(root, lefttree, righttree))
    ```
    - To determine the best feature, we use correlation. We compute it for every column which is expensive if there are 1000s of rows.
    - Seconds, to compute median, we first have to sort data that adds to the complexity.